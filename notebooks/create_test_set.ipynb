{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to original DeepFashion2 data\n",
    "TRAIN_IMG_DIR = \"../data/original/train/image\" \n",
    "TRAIN_ANNO_DIR = \"../data/original/train/annos\" \n",
    "VAL_IMG_DIR = \"../data/original/validation/image\" \n",
    "VAL_ANNO_DIR = \"../data/original/validation/annos\"\n",
    "\n",
    "# Paths to subsets\n",
    "TRAIN_CSV = \"../data/subset/train/classification_metadata.csv\" \n",
    "VAL_CSV = \"../data/subset/val/classification_metadata.csv\"\n",
    "\n",
    "# Paths to new test set\n",
    "TEST_IMG_DIR = \"../data/subset/test/images\" \n",
    "TEST_ANNO_DIR = \"../data/subset/test/annotations\" \n",
    "TEST_CSV = \"../data/subset/test/classification_metadata.csv\"\n",
    "\n",
    "CLASS_SAMPLE_COUNT = 100\n",
    "\n",
    "category_map = {\n",
    "    1:  \"short_sleeve_top\",\n",
    "    2:  \"long_sleeve_top\",\n",
    "    3:  \"short_sleeve_outwear\",\n",
    "    4:  \"long_sleeve_outwear\",\n",
    "    5:  \"vest\",\n",
    "    6:  \"sling\",\n",
    "    7:  \"shorts\",\n",
    "    8:  \"trousers\",\n",
    "    9:  \"skirt\",\n",
    "    10: \"short_sleeve_dress\",\n",
    "    11: \"long_sleeve_dress\",\n",
    "    12: \"vest_dress\",\n",
    "    13: \"sling_dress\"\n",
    "}\n",
    "\n",
    "random.seed(42) \n",
    "\n",
    "os.makedirs(TEST_IMG_DIR, exist_ok=True) \n",
    "os.makedirs(TEST_ANNO_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_used_images(csv_path):\n",
    "    used = set()\n",
    "    with open(csv_path, 'r', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            used.add(row[\"image_filename\"])\n",
    "    return used\n",
    "\n",
    "used_train = load_used_images(TRAIN_CSV)\n",
    "used_val   = load_used_images(VAL_CSV)\n",
    "used_images = used_train.union(used_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_item(annotation_data):\n",
    "    # Returns the category_id of the item with the largest bounding box area.\n",
    "    # Returns None if no items are found.\n",
    "    largest_area = 0\n",
    "    dominant_cat_id = None\n",
    "    i = 1\n",
    "    while True:\n",
    "        item_key = f\"item{i}\"\n",
    "        if item_key not in annotation_data:\n",
    "            break\n",
    "        item = annotation_data[item_key]\n",
    "        cat_id = item[\"category_id\"]\n",
    "        x1, y1, x2, y2 = item[\"bounding_box\"]\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "        if area > largest_area:\n",
    "            largest_area = area\n",
    "            dominant_cat_id = cat_id\n",
    "        i += 1\n",
    "    return dominant_cat_id\n",
    "\n",
    "def parse_dataset(img_dir, anno_dir):\n",
    "    # Only the 'dominant' item (largest bbox) is used to assign category_id.\n",
    "    category_index = defaultdict(list)\n",
    "    all_anno_files = [f for f in os.listdir(anno_dir) if f.endswith(\".json\")]\n",
    "    for anno_file in all_anno_files:\n",
    "        image_id = os.path.splitext(anno_file)[0]\n",
    "        img_file = image_id + \".jpg\"\n",
    "        anno_path = os.path.join(anno_dir, anno_file)\n",
    "        img_path  = os.path.join(img_dir, img_file)\n",
    "\n",
    "        # Skip if the image doesn't exist\n",
    "        if not os.path.isfile(img_path):\n",
    "            continue\n",
    "\n",
    "        with open(anno_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        cat_id = find_dominant_item(data)\n",
    "        if cat_id is not None:\n",
    "            category_index[cat_id].append((img_file, anno_file))\n",
    "    return category_index\n",
    "\n",
    "train_index = parse_dataset(TRAIN_IMG_DIR, TRAIN_ANNO_DIR)\n",
    "val_index   = parse_dataset(VAL_IMG_DIR, VAL_ANNO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_remaining(full_index, used_imgs):\n",
    "    # Remove any pairs whose img_file is in used_imgs\n",
    "    # Return the leftover as {cat_id -> [(img_file, anno_file), ...]}\n",
    "    remaining = {}\n",
    "    for cat_id, items in full_index.items():\n",
    "        leftover = []\n",
    "        for (img_file, anno_file) in items:\n",
    "            if img_file not in used_imgs:\n",
    "                leftover.append((img_file, anno_file))\n",
    "        if leftover:\n",
    "            remaining[cat_id] = leftover\n",
    "    return remaining\n",
    "\n",
    "leftover_train = filter_remaining(train_index, used_images)\n",
    "leftover_val   = filter_remaining(val_index, used_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_leftover = defaultdict(list)\n",
    "for cat_id, items in leftover_train.items():\n",
    "    for (img_file, anno_file) in items:\n",
    "        grouped_leftover[cat_id].append((cat_id, img_file, anno_file))\n",
    "for cat_id, items in leftover_val.items():\n",
    "    for (img_file, anno_file) in items:\n",
    "        grouped_leftover[cat_id].append((cat_id, img_file, anno_file))\n",
    "\n",
    "# For each category randomly select up to CLASS_SAMPLE_COUNT images\n",
    "sampled_items = []\n",
    "for cat_id, items in grouped_leftover.items():\n",
    "    random.shuffle(items)\n",
    "    count = min(CLASS_SAMPLE_COUNT, len(items))\n",
    "    sampled_items.extend(items[:count])\n",
    "\n",
    "random.shuffle(sampled_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating test subset with up to 100 images!\n",
      "Test CSV: ../data/subset/test/classification_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "def copy_test_items(sampled_list, test_csv_path):\n",
    "    \"\"\"\n",
    "    Copy each item into TEST_IMG_DIR/TEST_ANNO_DIR,\n",
    "    writing category_id and category_name in the CSV.\n",
    "    \"\"\"\n",
    "    with open(test_csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"image_filename\", \"annotation_filename\", \"category_id\", \"category_name\"])\n",
    "\n",
    "        for (cat_id, img_file, anno_file) in sampled_list:\n",
    "            cat_name = category_map.get(cat_id, f\"cat_{cat_id}\")\n",
    "            # Determine source directory (train or val)\n",
    "            if os.path.isfile(os.path.join(TRAIN_IMG_DIR, img_file)):\n",
    "                src_img_dir = TRAIN_IMG_DIR\n",
    "                src_anno_dir = TRAIN_ANNO_DIR\n",
    "            else:\n",
    "                src_img_dir = VAL_IMG_DIR\n",
    "                src_anno_dir = VAL_ANNO_DIR\n",
    "\n",
    "            src_img_path  = os.path.join(src_img_dir,  img_file)\n",
    "            src_anno_path = os.path.join(src_anno_dir, anno_file)\n",
    "            dst_img_path  = os.path.join(TEST_IMG_DIR,  img_file)\n",
    "            dst_anno_path = os.path.join(TEST_ANNO_DIR, anno_file)\n",
    "\n",
    "            # Copy files\n",
    "            shutil.copyfile(src_img_path, dst_img_path)\n",
    "            shutil.copyfile(src_anno_path, dst_anno_path)\n",
    "\n",
    "            # Write CSV row\n",
    "            writer.writerow([img_file, anno_file, cat_id, cat_name])\n",
    "\n",
    "copy_test_items(sampled_items, TEST_CSV)\n",
    "\n",
    "print(f\"Done creating test subset with up to {CLASS_SAMPLE_COUNT} images!\")\n",
    "print(f\"Test CSV: {TEST_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clothing-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21 (main, Dec 11 2024, 10:21:40) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "403f07482358ebae634d863ab8a0c33a06280500a45551c569a8f325de65c2a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
